{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9b9ccd-7610-4b37-b3e3-d7edeb134245",
   "metadata": {},
   "source": [
    "# Coursework 2: Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916beb2-7f91-4399-b9b1-d7116b60f130",
   "metadata": {},
   "source": [
    "This is the second coursework of ECS7023P Programming for AI and Data Science, which counts 65% towards the final grade of the module. The coursework is graded out of 100 marks.\n",
    "\n",
    "**Deadline:** Monday 28th October 2024 - 11.59pm\n",
    "\n",
    "**Marking criteria:** While the most important marking criterion will be for the code to achieve the expected objective and output, marks will also be given for partial or close solutions, whereas marks can be deducted for code that is overly complex, inefficient, difficult to understand and/or to maintain.\n",
    "\n",
    "**Use of packages:** In addition to built-in python functions and elements that we have seen in the lectures (see lecture notes), the only additional packages allowed for this exercise include *string* and *json* (unless otherwise stated in a specific question). No other packages are allowed. You cannot use other packages such as **pandas**, you won't get any marks for a question if you use it.\n",
    "\n",
    "**How to submit:** You will submit a completed Jupyter notebook file with your solutions, as well as a PDF version of the Jupyter notebook which includes the outputs of your code (either two separate files or a single zip file that contains both the .ipynb and .pdf files). Your submission will include the python code that produces the required answers. Answers produced through means other than python code will not be deemed acceptable.\n",
    "\n",
    "**Note:** This is an individual coursework, the solutions you submit need to be your own and developed on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6578f-608f-4b80-842e-3074b9898f97",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb580e29-0a76-4622-aadc-3f58f4d352f7",
   "metadata": {},
   "source": [
    "For this exercise, you are given a dataset that contains a sample of Wikipedia articles, with their content reduced to only text (that is with images and other non-textual content removed).\n",
    "\n",
    "The dataset is contained in a file called 'wiki-articles.json', where each line contains an entry with a Wikipedia article: an ID, a URL, title and body text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730edfcc-45bd-4bb3-b3fe-84c5da77b2ec",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454806b-7985-4166-bad1-6b9ebeb12a82",
   "metadata": {},
   "source": [
    "**Note:** For this coursework, you should remove all the punctuation prior to processing the text. To do this, use the following code, where *text* is the variable where you want to remove the punctuation:\n",
    "\n",
    "```\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text = text.translate(translator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb33e9-131f-424b-988d-10f754ef5add",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f133d-4f4b-4fd7-a287-e2078c8eb86e",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563884d-ddf3-42ab-9278-881609c21a05",
   "metadata": {},
   "source": [
    "1. Ignoring the case, what is the most frequently occurring character across all article titles and how many occurrences does it have? **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0b6330-0bcd-4b95-8d11-fa845e7e89eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character \"e\" is the most frequent character in the titles with a count of 25517\n",
      "{'a': 25319, 'n': 20100, 'r': 18392, 'c': 11111, 'h': 7472, 'i': 23253, 's': 16120, 'm': 9490, 'l': 12885, 'b': 4775, 'e': 25517, 'd': 7983, 'o': 21033, ' ': 23004, 't': 17861, 'p': 6771, 'y': 4412, 'w': 2055, 'f': 5256, 'u': 8071, 'g': 6936, 'k': 2510, 'x': 834, 'v': 2402, '1': 1678, '8': 554, 'q': 382, 'j': 999, 'z': 743, 'ó': 13, 'é': 75, 'ü': 23, 'à': 4, '9': 575, '0': 910, '4': 606, '7': 545, '–': 95, 'è': 12, 'á': 30, 'ā': 4, '3': 556, 'æ': 9, '2': 601, '5': 608, '6': 577, 'ë': 5, 'ï': 1, 'ʻ': 1, 'ç': 14, 'ö': 38, 'í': 26, 'ʼ': 2, 'ı': 2, 'ş': 1, 'ś': 1, 'ø': 9, 'ł': 14, 'â': 3, 'ș': 1, 'ã': 8, 'ḥ': 1, 'δ': 1, 'ő': 1, 'ą': 1, 'ō': 31, 'ğ': 3, 'ū': 6, 'ñ': 4, 'ä': 11, 'ń': 9, 'ć': 2, 'č': 1, 'ð': 7, '̇': 3, 'ú': 2, 'đ': 2, 'α': 2, 'ô': 1, 'ż': 2, 'ò': 4, 'ŵ': 1, '½': 1, 'š': 1, '♯': 2, 'ê': 1, 'σ': 1, 'ý': 1, 'ấ': 1, 'ạ': 1, 'å': 2, '×': 1, 'ň': 1, 'ì': 1, 'ę': 1}\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "character_dict = {}\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        clean_title = remove_punctuation(article.get('title','')).lower()\n",
    "        for character in clean_title:\n",
    "            if character:\n",
    "                if character not in character_dict.keys():\n",
    "                    character_dict[character] = 1\n",
    "                else:\n",
    "                    character_dict[character] += 1\n",
    "max_character = max(character_dict, key=character_dict.get)\n",
    "max_count = character_dict[max_character]\n",
    "\n",
    "print(f'The character \"{max_character}\" is the most frequent character in the titles with a count of {max_count}')\n",
    "print(character_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f3cf5-33ae-4aaf-b701-cb66ce752eeb",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228570-a35e-451d-b395-688ffbeee63c",
   "metadata": {},
   "source": [
    "2. The least frequent characters in English are: x, z, j, q. Ignoring the case, how many articles are there in the dataset that do NOT contain any of these 4 characters in the body texts? **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd10f3e-3da6-4202-8208-2eaa56c6037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n",
      "CPU times: total: 1.16 s\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''Have not removed punctuation from the text as (a) it leads to the cell taking significantly longer to evaluate and (b) removal of punctuation is\n",
    "unnecessary for counting whether a certain character occurs in the text'''\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "least_freq_set = {'x', 'z', 'j', 'q'}\n",
    "counter = 0\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        clean_text = article.get('text', '').lower()\n",
    "\n",
    "        if not any(char in least_freq_set for char in clean_text):\n",
    "            counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914ba6e-d487-47ff-88d9-bcf65c2c455a",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cee4d0-5cf0-405a-a9df-7268fbc459e3",
   "metadata": {},
   "source": [
    "3. What is the longest word in the dataset that starts with a vowel (a, e, i, o, u) and ends with an 's'? NB: ignore the case. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf5d6279-6c22-4292-b932-20f9e9743ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernesthemingwayfromrealitytofictionafarewelltoarms\n",
      "CPU times: total: 1min 22s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I consider it sufficient to check the 'text' and 'title' columns. \n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "longest_word = \"\"\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        text = article.get('text', '').translate(translator).lower()\n",
    "        title = article.get('title', '').translate(translator).lower()\n",
    "\n",
    "        for word in text.split():\n",
    "            if word[0] in vowels and word.endswith('s'):\n",
    "                if len(word) > len(longest_word):\n",
    "                    longest_word = word\n",
    "        for word in title.split():\n",
    "            if word[0] in vowels and word.endswith('s'):\n",
    "                if len(word) > len(longest_word):\n",
    "                    longest_word = word\n",
    "\n",
    "print(longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26892355-ad95-4098-923a-7672d14799a9",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96dfa87-3678-4ef5-944c-3b51f2e41d9e",
   "metadata": {},
   "source": [
    "4. If we replaced vowels (a, e, i, o, u) with wild cards (\\_), we would say that, for example, 'hat', 'hit' and 'hot' translate into the same wild card word, i.e. 'h_t'. And based on this we would say that the wild card word 'h_t' has 3 different variations in our dataset, 'hat', 'hit' and 'hot'. What is the largest set of words in the dataset which translate into the exact same wild card word? NB: ignore the case. **(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27425da2-db5b-4a67-9a97-0d1a2627b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wildcard pattern '___' has the largest set with 91 variations.\n",
      "Words translated into '___': {'ueo', 'uei', 'euu', 'uua', 'aau', 'eaa', 'ieu', 'oao', 'aia', 'iie', 'uau', 'aio', 'eia', 'aae', 'iai', 'oua', 'ooo', 'ooi', 'oei', 'iou', 'aoa', 'oia', 'eii', 'oai', 'uea', 'iee', 'aei', 'iuu', 'aue', 'aeo', 'eea', 'eee', 'aeu', 'oie', 'aiu', 'iau', 'auu', 'aee', 'oee', 'iae', 'oea', 'eae', 'iue', 'uio', 'aie', 'eui', 'aii', 'aoi', 'eue', 'eeo', 'eai', 'eoa', 'eei', 'aua', 'iao', 'uee', 'uai', 'uaa', 'ooa', 'eoi', 'oeu', 'aou', 'aoe', 'euo', 'iui', 'oue', 'aaa', 'oio', 'oui', 'ioe', 'iaa', 'uue', 'ioi', 'iii', 'iea', 'uuu', 'oeo', 'oau', 'eau', 'oaa', 'uiu', 'oiu', 'aea', 'iia', 'iei', 'aui', 'uae', 'eua', 'aai', 'uia', 'eiu'}\n",
      "CPU times: total: 6min 40s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# I consider it sufficient to check the 'text' and 'title' columns. \n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "vowels = set('aeiou')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "wildcard_dict = {}\n",
    "largest_wildcard = None\n",
    "largest_set_length = 0\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        text = article.get('text', '').translate(translator).lower()\n",
    "        title = article.get('text', '').translate(translator).lower()\n",
    "\n",
    "        for word in text.split():\n",
    "            wildcard = ''.join('_' if char in vowels else char for char in word)\n",
    "\n",
    "            if wildcard in wildcard_dict:\n",
    "                wildcard_dict[wildcard].add(word)\n",
    "            else:\n",
    "                wildcard_dict[wildcard] = {word}\n",
    "\n",
    "            current_set_length = len(wildcard_dict[wildcard])\n",
    "            if current_set_length > largest_set_length:\n",
    "                largest_set_length = current_set_length\n",
    "                largest_wildcard = wildcard\n",
    "\n",
    "        for word in title.split():\n",
    "            wildcard = ''.join('_' if char in vowels else char for char in word)\n",
    "\n",
    "            if wildcard in wildcard_dict:\n",
    "                wildcard_dict[wildcard].add(word)\n",
    "            else:\n",
    "                wildcard_dict[wildcard] = {word}\n",
    "\n",
    "            current_set_length = len(wildcard_dict[wildcard])\n",
    "            if current_set_length > largest_set_length:\n",
    "                largest_set_length = current_set_length\n",
    "                largest_wildcard = wildcard\n",
    "\n",
    "if largest_wildcard is not None:\n",
    "    print(f\"The wildcard pattern '{largest_wildcard}' has the largest set with {largest_set_length} variations.\")\n",
    "    print(f\"Words translated into '{largest_wildcard}': {wildcard_dict[largest_wildcard]}\")\n",
    "else:\n",
    "    print(\"No wildcard patterns found in this dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab683620-96df-49b3-b2b3-e09f23b70713",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8290dbb-f256-486f-89a9-481eddf1eeaf",
   "metadata": {},
   "source": [
    "5. Two words are anagrams of each other if they can be written using the same characters but in a different order, including repeated characters. For example, 'cheap' is an anagram of 'peach', 'reacting' is an anagram of 'creating' and 'resistance' is an anagram of 'ancestries'.\n",
    "\n",
    "Likewise, **n-word anagram sets** include **n** words that are anagrams of one another:\n",
    "- 'asleep', 'please' and 'elapse' form a 3-word anagram set.\n",
    "- 'aspired', 'despair', 'diapers' and 'praised' form a 4-word anagram set.\n",
    "\n",
    "By only considering words that are *6 characters or longer* within article body texts, what is the largest anagram set? i.e. what is the *n* for this anagram set and which anagram words does it include. Note: we are also interested in checking anagram sets across articles, not only within articles. NB: ignore the case. **(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8256db0-1c00-419a-be84-e8c494f1c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest anagram set has 43 words: ['lushqoxdmznaikfrepcybwvgtj', 'kgwntrblqpahydvjifxezocsmu', 'uorytqslwxzhnmbvfcgeapijdk', 'hlnrskjamgfbicuqpdeyozxwtv', 'kptxigfmesauhyqbovjclrzdnw', 'xdybpwosmuzriqgenlhvjtfack', 'dliajuovcexbnmgqpwzyfhrkts', 'jkgoptcihabrnmdeylzfxwvuqs', 'gcbuzrasyxvmlpqnofhwdktjie', 'xpjuowiygcvrtqebnlzmdkfahs', 'disauyombpnthkgjrqclezxwfv', 'fjlvqakxnbgcpirmeoyzwduhst', 'ktjuqonpzcamlgfhewxbdyrsvi', 'zqxuvgfnwrlkphtmbjyodeicsa', 'xjwfrdzsqblktvpoiehmyncaug', 'fsktjarxpecnulyizgbdmwvhoq', 'ceakbmryuvdnfltxwgzoijqphs', 'tljrvqhgucxbzyswfdoaiepknm', 'yhlpgtebkwicsvudrqmfonjzax', 'krulgjewnfadvipoybxzcmhsqt', 'rcbpqmvzxyuofsldeanwkgtijh', 'fcbjqawtvdynxlusezphoigmkr', 'vftqsbporuzwyxhgdiecjalnmk', 'jsrhfenduazyqgxtmcbpiwvolk', 'rcbutxvzjinqpkwmlayedgofsh', 'urfxncmylvpigesktboqajzdhw', 'jiozfewmbaushpcnrqlvktgyxd', 'zgvrkobxlneiwjfusdqypcmhta', 'rmjvlyqzkciebonugawxpdstfh', 'gkqrfeanzpbmlhvjcduxsoytwi', 'ymztgvekqohpbsjliundrfxwac', 'pdsbtiuqfnovwjkahzceglmyxr', 'abcdefghijklmnopqrstuvwxyz', 'efmqabguinkxcjordpzthwvlys', 'ofrjvmazhqnbxpykculgswetdi', 'nukchvsmdgtzqfyewpialoxrjb', 'xjmiyvcarqowhlndsufkgbepzt', 'qungalxepkzyrdsoftvcmbihwj', 'rdobjntkvehmlfcwzaxgyipsuq', 'evtnhqdxwzjfucpiamorbsyglk', 'hvgpwsumdbtncokxjiqzrflaey', 'tzdipnjesycuhavrmxgkbfqwol', 'glqywbtizdpsfkanjcuxrevmoh']\n",
      "CPU times: total: 2min 3s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import string\n",
    "\n",
    "anagram_dict = {}\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "largest_n = 0\n",
    "largest_anagram_set = []\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        text = article.get('text', '').translate(translator).lower()\n",
    "\n",
    "        for word in text.split():\n",
    "            if len(word) >= 6: \n",
    "                sorted_characters = tuple(sorted(word))\n",
    "                \n",
    "                if sorted_characters not in anagram_dict:\n",
    "                    anagram_dict[sorted_characters] = []\n",
    "                if word not in anagram_dict[sorted_characters]:\n",
    "                    anagram_dict[sorted_characters].append(word)\n",
    "\n",
    "                    current_set_length = len(anagram_dict[sorted_characters])\n",
    "                    if current_set_length > largest_n:\n",
    "                        largest_n = current_set_length\n",
    "                        largest_anagram_set = anagram_dict[sorted_characters]\n",
    "\n",
    "if largest_anagram_set:\n",
    "    print(f\"The largest anagram set has {largest_n} words: {largest_anagram_set}\")\n",
    "else:\n",
    "    print(\"No anagram sets found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3ed4d-945b-48f9-845c-554ece0df9fd",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0762e11-be5e-422a-ac41-fbb0e714d7a8",
   "metadata": {},
   "source": [
    "6. TF-IDF is a metric that allows us to determine the saliency of a word, considering how frequent it is in a document (term frequency or TF) and how infrequent it is in a dataset (inverse document frequency or IDF). If we have a toy dataset consisting of 3 documents:\n",
    "\n",
    "* Doc \\#1: The house on the hill.\n",
    "* Doc \\#2: The weather in the UK on Christmas Day.\n",
    "* Doc \\#3: The house in Cornwall.\n",
    "\n",
    "TF-IDF is calculated as the multiplication between TF and IDF: TF-IDF = TF * IDF\n",
    "\n",
    "Where: TF = number of occurrences of a word in a document (e.g. 'the' occurring twice in doc \\#1).\n",
    "\n",
    "and $IDF = log(N / n_t)$, that's the logarithm of N (total number of documents in the dataset, i.e. 3) divided by $n_t$, which is the number of documents where the word is present (i.e. $n_t$ is 3 for 'the' and 2 for 'house')\n",
    "\n",
    "If we are looking for TF-IDF scores in doc \\#1, it has 4 words (the, house, on, hill) and their TF-IDF scores are as follows:\n",
    "* TF(the) = 2; IDF(the) = log(3/3) = 0; ***TF-IDF(the) = 2 * 0 = 0***\n",
    "* TF(house) = 1; IDF(house) = log(3/2) = 0.1761; ***TF-IDF(house) = 1 * 0.1761 = 0.1761***\n",
    "* TF(on) = 1; IDF(on) = log(3/2) = 0.1761; ***TF-IDF(on) = 1 * 0.1761 = 0.1761***\n",
    "* TF(hill) = 1; IDF(hill) = log(3/1) = 0.4771; ***TF-IDF(hill) = 1 * 0.4771 = 0.4771***\n",
    "\n",
    "Having computed all TF-IDF scores for doc \\#1, we would conclude that the word with the highest TF-IDF score is 'hill', with a score of 0.4771. That is, we consider it the most salient word in the document taking into account the characteristics of the dataset, 'the' is more frequent per se in the document alone, but given how frequent 'the' is in the whole dataset, it's less unique which reduces its score.\n",
    "\n",
    "Note: TF is specific to each word in each document, whereas IDF is consistent for each word across the entire dataset.\n",
    "\n",
    "Write a python function which, given an article ID as input, outputs the word within that article with the highest TF-IDF score. Print the word itself and its TF-IDF score. Consider only article body texts for this question, don't use the title text, and you should also ignore the case. As an example, show the output of your function for the article ID 593.\n",
    "\n",
    "**(15 marks)**\n",
    "\n",
    "NB: for this question, in addition to the packages allowed above, you can import the following:\n",
    "\n",
    "```\n",
    "from math import log\n",
    "from collections import Counter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d89978d9-58ac-4a3f-97f3-8831da2769a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word with the highest TF-IDF in article 593: 'animation' with a score of 412.3085\n",
      "CPU times: total: 1min 20s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def find_highest_tfidf(article_id):\n",
    "    total_docs = 0\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    word_dict = {}\n",
    "\n",
    "    with open('wiki-articles.json', 'r') as file:\n",
    "        for line in file:\n",
    "            article = json.loads(line)\n",
    "            total_docs += 1\n",
    "\n",
    "            if article.get(\"id\") == str(article_id):\n",
    "                target_text = article.get(\"text\", \"\").translate(translator).lower().split()\n",
    "                word_counts = Counter(target_text)\n",
    "                word_dict = {key: 0 for key in word_counts.keys()}\n",
    "\n",
    "    word_set = set(word_counts.keys())\n",
    "\n",
    "    with open('wiki-articles.json', 'r') as file:\n",
    "        for line in file:\n",
    "            article = json.loads(line)\n",
    "            text = article.get('text', '').translate(translator).lower().split()\n",
    "            text_set = set(text)\n",
    "\n",
    "            for word in word_set: \n",
    "                if word in text_set: \n",
    "                    word_dict[word] += 1\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_word = None\n",
    "\n",
    "    for word, tf in word_counts.items():\n",
    "        idf = log(total_docs / word_dict[word]) if word_dict[word] > 0 else 0\n",
    "        tf_idf = tf * idf\n",
    "        if tf_idf > highest_score:\n",
    "            highest_score = tf_idf\n",
    "            highest_word = word\n",
    "\n",
    "    print(f\"Word with the highest TF-IDF in article {article_id}: '{highest_word}' with a score of {highest_score:.4f}\")\n",
    "\n",
    "find_highest_tfidf(\"593\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b00e9-e056-411a-8958-f4b57d0d0022",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d78e23-6cd3-4394-9a98-057194df1120",
   "metadata": {},
   "source": [
    "7. We are looking for the longest sequence of words across article body texts which alternates vowels (a, e, i, o, u) and consonants (remainder of alphabetic characters). Examples of words that alternate vowels and consonants are 'unapologetic' and 'cider'. When looking at sequences of words alternating vowels and consonants, we will ignore any spaces in the sequence and look at the sequence of characters across those words. An example of a sequence of words alternating vowels and consonants is 'buy a bike now', but 'unapologetic cider' isn't. When looking for the longest sequence, we count the length in number of characters and not in number of words, e.g. 'buy a bike now' has a length of 11 characters (not a length of 4 words, or a length of 14 characters counting spaces).\n",
    "\n",
    "Write the python code that finds the longest sequence of words across all article body texts which alternates vowels and consonants, and print both this sequence and its length. **(20 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb85a2b3-f626-4935-829e-96598fb239c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest alternating sequence: 'hem are lalolalo lano lanutavake lanutuli lanumaha kikila' with length 57\n",
      "CPU times: total: 4min 53s\n",
      "Wall time: 9min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "longest_sequence = []\n",
    "current_sequence = []\n",
    "last_char = None\n",
    "consonants = set('bcdfghjklmnpqrstvwxyz')\n",
    "vowels = set('aeiou')\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        text = article.get('text', '').translate(translator).lower()\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            if not current_sequence:\n",
    "                current_sequence.append(char)\n",
    "                last_char = char\n",
    "                continue\n",
    "\n",
    "            if ((last_char in vowels and char in consonants) or \n",
    "                (last_char in consonants and char in vowels)):\n",
    "                current_sequence.append(char)\n",
    "                last_char = char\n",
    "            elif char == ' ' and i + 1 < len(text) and (\n",
    "                    (last_char in vowels and text[i + 1] in consonants) or \n",
    "                    (last_char in consonants and text[i + 1] in vowels)):\n",
    "                current_sequence.append(char)\n",
    "            else:\n",
    "                if len(current_sequence) > len(longest_sequence):\n",
    "                    longest_sequence = current_sequence\n",
    "                current_sequence = [char]\n",
    "                last_char = char\n",
    "\n",
    "        if len(current_sequence) > len(longest_sequence):\n",
    "            longest_sequence = current_sequence\n",
    "\n",
    "\n",
    "print(f\"Longest alternating sequence: '{''.join(longest_sequence)}' with length {len(longest_sequence)}\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f84c1-11d6-4f62-add2-aa63b3278608",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce0c49-0713-4b9b-848c-5a945ee1cedb",
   "metadata": {},
   "source": [
    "8. With the aim of building a small search engine, we want to index the contents of the articles in the dataset (i.e. mapping which articles contain each word), where we also want to store the position(s) in which content appears in each article. For example, if an article ID is 7 and its text is \"python coding is so much fun\", we would say that the word 'coding' appears in position 2 of article ID 7 and the word 'fun' appears in position 5 of article ID 7. A toy example of an index could be:\n",
    "- 'house' can be found in: position 97 of article 7, positions 32 and 221 of article 13 and position 63 of article 27.\n",
    "- 'London' can be found in: positions 17 and 97 of article 7, position 21 of article 25 and position 157 of article 42.\n",
    "- etc.\n",
    "\n",
    "NB: we will be ignoring the case in this entire exercise #8.\n",
    "\n",
    "**(20 marks total)** (see specific marks for each of the subquestions of question 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f79859-a0a7-432d-bab6-068262a3aae7",
   "metadata": {},
   "source": [
    "8. (a) Create a data structure that indexes the dataset contents following the above objective. **(10 marks)**\n",
    "\n",
    "**Note:** questions 8b and 8c need to be implemented using the indexing structure created here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de95b496-b9b6-46bb-9832-e230b44a5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 33s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The two examples from the sentence 'python coding is so much fun' show different indexing styles. The first is 1-indexed, the second 0-indexed.\n",
    "# I use 0-indexing\n",
    "\n",
    "import json\n",
    "import string\n",
    "\n",
    "index_dict = {}\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "with open('wiki-articles.json', 'r') as file:\n",
    "    for line in file:\n",
    "        article = json.loads(line)\n",
    "        article_id = int(article.get(\"id\", ''))\n",
    "        text = article.get(\"text\", '').translate(translator).lower()\n",
    "        \n",
    "        for index, word in enumerate(text.split()):\n",
    "            if word not in index_dict:\n",
    "                index_dict[word] = {article_id: {index}}\n",
    "            else:\n",
    "                if article_id not in index_dict[word]:\n",
    "                    index_dict[word][article_id] = {index}\n",
    "                else:\n",
    "                    index_dict[word][article_id].add(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f146e9-2095-42c1-8326-baf4f3f832ab",
   "metadata": {},
   "source": [
    "8. (b) Given a word as input parameter, write a function that outputs the article IDs in which the word can be found. As an example, produce the output for the word 'python'. **(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22408646-f2f9-4160-a7fb-2ead2827f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[594, 809, 1063, 1437, 2581, 2807, 3054, 3382, 4015, 4147, 4373, 4579, 4653, 4668, 4706, 4887, 4893, 5213, 5644, 5700, 5739, 6021, 6667, 6696, 6698, 6974, 7220, 7392, 7574, 7575, 7951, 7962, 8091, 8267, 8531, 8786, 8829, 8904, 8996, 9010, 9069, 9288, 9498, 9710, 9838, 10049, 10283, 10474, 10606, 10793, 10933, 11142, 11168, 11367, 11376, 11642, 11673, 11741, 11966, 12302, 12531, 12628, 12731, 13024, 13052, 13208, 13790, 13834, 14467, 14794, 14801, 15154, 15305, 15704, 15858, 16036, 16389, 16708, 16808, 17399, 17415, 17443, 17871, 17920, 18016, 18137, 18155, 18203, 18574, 18692, 18838, 18894, 18942, 18977, 19160, 19161, 19162, 19163, 19164, 19165, 19545, 19550, 19620, 19669, 19701, 20034, 20036, 20039, 20362, 20412, 21017, 21037, 21506, 21523, 21796, 21873, 22055, 22091, 22145, 22330, 22589, 22592, 22626, 22660, 22693, 22758, 22826, 23015, 23329, 23529, 23547, 23659, 23716, 23824, 23862, 23939, 24131, 24185, 24267, 24518, 24522, 24722, 24894, 25184, 25270, 25407, 25409, 25717, 25739, 25768, 25794, 26386, 26490, 27027, 27695, 27701, 27977, 28119, 28319, 28341, 28367, 28368, 28442, 28555, 28698, 28760, 28804, 28938, 29004, 29208, 29370, 29519, 29549, 29843, 30054, 30302, 30410, 30816, 30890, 31139, 31180, 31206, 31268, 31454, 31573, 31742, 32188, 32478, 33164, 33898, 33955, 33958, 34004, 34119, 34138, 34211, 34358, 34472, 36713, 36777, 37035, 37472, 37891, 38221, 38689, 38838, 39289, 40132, 40317, 41227]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def word_finder(word):\n",
    "    word = str(word)\n",
    "    word_dict = index_dict.get(word, '')\n",
    "    if word_dict:\n",
    "        return list(word_dict.keys())\n",
    "    else:\n",
    "        return f\"No such word in the dataset\"\n",
    "\n",
    "print(word_finder('python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2df18-cffc-438e-8a2d-736634cb32de",
   "metadata": {},
   "source": [
    "8. (c) Given two words and a distance value (integer) as input, return the list of article IDs where the two words occur within the distance. As an example, produce the output for words 'python' and 'coding' within a distance of 20. **(15 marks)**\n",
    "\n",
    "**For example,** if we have a dataset with five articles:\n",
    "- article ID 1: \"python coding\"\n",
    "- article ID 2: \"I am coding in python\"\n",
    "- article ID 3: \"To learn coding, I have decided to start with python\"\n",
    "- article ID 4: \"python is a fantastic programming language\"\n",
    "- article ID 5: \"it's sunny today and I like it\"\n",
    "\n",
    "If we are given two words: 'python' and 'coding', and a distance of 2:\n",
    "- article ID 1: the condition is true as 'python' and 'coding' occur within a distance of 1.\n",
    "- article ID 2: the condition is true as 'python' and 'coding' occur within a distance of 2.\n",
    "- article ID 3: the condition is false as 'python' and 'coding' occur within a larger distance of 7.\n",
    "- article ID 4: the condition is false because it only contains 'python', no 'coding'.\n",
    "- article ID 5: the condition is false because it doesn't contain any of the two keywords 'python' and 'coding'.\n",
    "\n",
    "Therefore, in this case, within_distance_ids('python', 'coding', 2) would output [1, 2] as the list of matching article IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce7550b5-e760-407f-8a34-fe2125bfde78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6974, 23862]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def article_per_distance_and_words(word_1=str, word_2=str, distance=int):\n",
    "    word_1_dict = index_dict.get(word_1, '')\n",
    "    word_2_dict = index_dict.get(word_2, '')\n",
    "    article_list = []\n",
    "    \n",
    "    for article_id_1, index_set_1 in word_1_dict.items():\n",
    "        if article_id_1 in word_2_dict.keys():\n",
    "            flag = False\n",
    "            \n",
    "            for index in index_set_1:\n",
    "                \n",
    "                for i in range(index-distance, index+distance+1):\n",
    "                    if i in word_2_dict[article_id_1]:\n",
    "                        article_list.append(article_id_1)\n",
    "                        flag = True\n",
    "                        break\n",
    "                if flag:\n",
    "                    break\n",
    "              \n",
    "    return article_list\n",
    "\n",
    "print(article_per_distance_and_words('python', 'coding', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73732deb-4db4-4343-8fc9-12de1f838128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
